{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poker AI Setup (Deep MCCFR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pokerenv.obs_indices as indices\n",
    "from pokerenv.table import Table\n",
    "from treys import Deck, Evaluator, Card\n",
    "from pokerenv.common import GameState, PlayerState, PlayerAction, TablePosition, Action, action_list\n",
    "from pokerenv.player import Player\n",
    "from pokerenv.utils import pretty_print_hand, approx_gt, approx_lte\n",
    "import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from pokerenv import Poker\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "class PokerStateEncoder:\n",
    "    \"\"\"Handles state encoding for poker observations\"\"\"\n",
    "    def __init__(self, num_cards: int = 52):\n",
    "        self.num_cards = num_cards\n",
    "        \n",
    "    def encode_cards(self, cards: List[int]) -> np.ndarray:\n",
    "        \"\"\"One-hot encode cards\"\"\"\n",
    "        encoding = np.zeros(self.num_cards)\n",
    "        for card in cards:\n",
    "            if card is not None:  # Handle hidden cards\n",
    "                encoding[card] = 1\n",
    "        return encoding\n",
    "    \n",
    "    def encode_state(self, obs: dict) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encode full poker state\n",
    "        Expected obs keys: hole_cards, community_cards, pot, current_bet, \n",
    "                         position, betting_history\n",
    "        \"\"\"\n",
    "        # Encode cards\n",
    "        hole_cards_enc = self.encode_cards(obs['hole_cards'])\n",
    "        community_cards_enc = self.encode_cards(obs['community_cards'])\n",
    "        \n",
    "        # Encode numerical values\n",
    "        pot_enc = np.array([obs['pot'] / 1000.0])  # Normalize pot\n",
    "        bet_enc = np.array([obs['current_bet'] / 1000.0])  # Normalize bet\n",
    "        \n",
    "        # Encode position\n",
    "        position_enc = np.zeros(6)  # Assuming 6-max table\n",
    "        position_enc[obs['position']] = 1\n",
    "        \n",
    "        # Encode betting history (last 4 actions)\n",
    "        history_enc = np.zeros(3 * 4)  # 3 possible actions * 4 last actions\n",
    "        for i, action in enumerate(obs['betting_history'][-4:]):\n",
    "            history_enc[i * 3 + action] = 1\n",
    "            \n",
    "        # Concatenate all features\n",
    "        return np.concatenate([\n",
    "            hole_cards_enc,\n",
    "            community_cards_enc,\n",
    "            pot_enc,\n",
    "            bet_enc,\n",
    "            position_enc,\n",
    "            history_enc\n",
    "        ])\n",
    "\n",
    "class PopulationManager:\n",
    "    \"\"\"Manages a population of agents for training\"\"\"\n",
    "    def __init__(self, num_agents: int, env: Poker):\n",
    "        self.agents = [DeepMCCFR(env) for _ in range(num_agents)]\n",
    "        self.performance_history = defaultdict(list)\n",
    "        \n",
    "    def select_training_pair(self) -> Tuple[int, int]:\n",
    "        \"\"\"Select two agents for training\"\"\"\n",
    "        idx1, idx2 = random.sample(range(len(self.agents)), 2)\n",
    "        return idx1, idx2\n",
    "    \n",
    "    def update_performance(self, agent_idx: int, reward: float):\n",
    "        \"\"\"Track agent performance\"\"\"\n",
    "        self.performance_history[agent_idx].append(reward)\n",
    "    \n",
    "    def merge_experiences(self, frequency: int = 1000):\n",
    "        \"\"\"Periodically merge experiences across successful agents\"\"\"\n",
    "        if len(self.performance_history[0]) % frequency == 0:\n",
    "            # Get top performing agents\n",
    "            avg_performances = {\n",
    "                idx: np.mean(perfs[-100:])\n",
    "                for idx, perfs in self.performance_history.items()\n",
    "            }\n",
    "            top_agents = sorted(\n",
    "                avg_performances.keys(),\n",
    "                key=lambda x: avg_performances[x],\n",
    "                reverse=True\n",
    "            )[:3]\n",
    "            \n",
    "            # Share experiences among top agents\n",
    "            for idx1 in top_agents:\n",
    "                for idx2 in top_agents:\n",
    "                    if idx1 != idx2:\n",
    "                        self._share_experiences(idx1, idx2)\n",
    "    \n",
    "    def _share_experiences(self, agent1_idx: int, agent2_idx: int):\n",
    "        \"\"\"Share experiences between two agents\"\"\"\n",
    "        agent1 = self.agents[agent1_idx]\n",
    "        agent2 = self.agents[agent2_idx]\n",
    "        \n",
    "        # Share advantage memories\n",
    "        for player in range(2):\n",
    "            memories1 = agent1.advantage_memories[player].sample(1000)\n",
    "            memories2 = agent2.advantage_memories[player].sample(1000)\n",
    "            \n",
    "            for memory in memories1:\n",
    "                agent2.advantage_memories[player].add(memory)\n",
    "            for memory in memories2:\n",
    "                agent1.advantage_memories[player].add(memory)\n",
    "\n",
    "class DeepMCCFR:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: Poker,\n",
    "        hidden_size: int = 256,\n",
    "        learning_rate: float = 0.001,\n",
    "        memory_size: int = 1000000,\n",
    "        batch_size: int = 32,\n",
    "        update_freq: int = 100\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.state_encoder = PokerStateEncoder()\n",
    "        \n",
    "        # Calculate input size based on encoded state\n",
    "        sample_obs = env.reset()\n",
    "        encoded_state = self.state_encoder.encode_state(sample_obs)\n",
    "        self.input_size = len(encoded_state)\n",
    "        \n",
    "        # Network initialization (rest remains same as before)\n",
    "        self.advantage_nets = [\n",
    "            PokerNetwork(self.input_size, hidden_size, env.action_space.n)\n",
    "            for _ in range(2)\n",
    "        ]\n",
    "        \n",
    "        \n",
    "\n",
    "    def get_state_tensor(self, obs: dict) -> torch.Tensor:\n",
    "        \"\"\"Convert observation to tensor format\"\"\"\n",
    "        encoded_state = self.state_encoder.encode_state(obs)\n",
    "        return torch.FloatTensor(encoded_state).unsqueeze(0)\n",
    "\n",
    "    def traverse_game_tree(self, player: int, traverser: int):\n",
    "        \"\"\"Modified to handle proper state encoding\"\"\"\n",
    "        if self.env.is_terminal():\n",
    "            return self.env.get_payoff(traverser)\n",
    "        \n",
    "        obs = self.env.get_observation()  # Get current observation\n",
    "        legal_actions = self.env.legal_actions()\n",
    "        \n",
    "        if player == traverser:\n",
    "            strategy = self.get_strategy(obs, player)\n",
    "            advantages = np.zeros(self.env.action_space.n)\n",
    "            \n",
    "            for action in legal_actions:\n",
    "                self.env.step(action)\n",
    "                payoff = self.traverse_game_tree(1 - player, traverser)\n",
    "                self.env.undo_step()\n",
    "                \n",
    "                advantages[action] = payoff\n",
    "            \n",
    "            # Store encoded state\n",
    "            self.advantage_memories[player].add(\n",
    "                (self.state_encoder.encode_state(obs), advantages, legal_actions)\n",
    "            )\n",
    "            \n",
    "            contribution = advantages.dot(strategy)\n",
    "            return contribution\n",
    "            \n",
    "        else:\n",
    "            strategy = self.get_strategy(obs, player)\n",
    "            # ... (rest remains same as before)\n",
    "\n",
    "def train_population(\n",
    "    num_agents: int,\n",
    "    env: Poker,\n",
    "    num_iterations: int,\n",
    "    merge_freq: int = 1000\n",
    "):\n",
    "    \"\"\"Train a population of agents\"\"\"\n",
    "    pop_manager = PopulationManager(num_agents, env)\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        # Select two agents to play against each other\n",
    "        idx1, idx2 = pop_manager.select_training_pair()\n",
    "        agent1 = pop_manager.agents[idx1]\n",
    "        agent2 = pop_manager.agents[idx2]\n",
    "        \n",
    "        # Play one game\n",
    "        env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            current_player = env.current_player()\n",
    "            current_agent = agent1 if current_player == 0 else agent2\n",
    "            \n",
    "            obs = env.get_observation()\n",
    "            action = current_agent.act(obs, current_player)\n",
    "            _, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Update performance history\n",
    "        pop_manager.update_performance(idx1, reward if current_player == 0 else -reward)\n",
    "        pop_manager.update_performance(idx2, -reward if current_player == 0 else reward)\n",
    "        \n",
    "        # Periodically merge experiences\n",
    "        if iteration % merge_freq == 0:\n",
    "            pop_manager.merge_experiences()\n",
    "            \n",
    "        # Train both agents\n",
    "        agent1.train_advantage_network(0)\n",
    "        agent1.train_strategy_network(0)\n",
    "        agent2.train_advantage_network(1)\n",
    "        agent2.train_strategy_network(1)\n",
    "    \n",
    "    return pop_manager\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    env = Poker()\n",
    "    pop_manager = train_population(\n",
    "        num_agents=10,\n",
    "        env=env,\n",
    "        num_iterations=100000,\n",
    "        merge_freq=1000\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play against Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "import numpy as np\n",
    "from pokerenv import Poker\n",
    "from multi_agent_mccfr import DeepMCCFR, PopulationManager  # Import from your previous code\n",
    "\n",
    "class PokerGame:\n",
    "    \"\"\"Interface for playing against trained agent\"\"\"\n",
    "    def __init__(self, agent: DeepMCCFR, env: Poker):\n",
    "        self.agent = agent\n",
    "        self.env = env\n",
    "        self.card_symbols = {\n",
    "            0: \"2♠\", 1: \"3♠\", 2: \"4♠\", 3: \"5♠\", 4: \"6♠\", 5: \"7♠\", \n",
    "            6: \"8♠\", 7: \"9♠\", 8: \"10♠\", 9: \"J♠\", 10: \"Q♠\", 11: \"K♠\", 12: \"A♠\",\n",
    "            13: \"2♣\", 14: \"3♣\", 15: \"4♣\", 16: \"5♣\", 17: \"6♣\", 18: \"7♣\",\n",
    "            19: \"8♣\", 20: \"9♣\", 21: \"10♣\", 22: \"J♣\", 23: \"Q♣\", 24: \"K♣\", 25: \"A♣\",\n",
    "            26: \"2♥\", 27: \"3♥\", 28: \"4♥\", 29: \"5♥\", 30: \"6♥\", 31: \"7♥\",\n",
    "            32: \"8♥\", 33: \"9♥\", 34: \"10♥\", 35: \"J♥\", 36: \"Q♥\", 37: \"K♥\", 38: \"A♥\",\n",
    "            39: \"2♦\", 40: \"3♦\", 41: \"4♦\", 42: \"5♦\", 43: \"6♦\", 44: \"7♦\",\n",
    "            45: \"8♦\", 46: \"9♦\", 47: \"10♦\", 48: \"J♦\", 49: \"Q♦\", 50: \"K♦\", 51: \"A♦\"\n",
    "        }\n",
    "    \n",
    "    def format_cards(self, cards: List[int]) -> str:\n",
    "        \"\"\"Convert card IDs to readable format\"\"\"\n",
    "        return \" \".join(self.card_symbols[c] for c in cards if c is not None)\n",
    "    \n",
    "    def display_game_state(self, obs: Dict):\n",
    "        \"\"\"Display current game state\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Pot: ${obs['pot']}\")\n",
    "        print(f\"Current bet: ${obs['current_bet']}\")\n",
    "        print(f\"Your hole cards: {self.format_cards(obs['hole_cards'])}\")\n",
    "        \n",
    "        community_cards = obs['community_cards']\n",
    "        if any(card is not None for card in community_cards):\n",
    "            print(f\"Community cards: {self.format_cards(community_cards)}\")\n",
    "        \n",
    "        print(f\"Your stack: ${obs['stack']}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    def get_human_action(self) -> int:\n",
    "        \"\"\"Get action from human player\"\"\"\n",
    "        legal_actions = self.env.legal_actions()\n",
    "        action_map = {\n",
    "            0: \"Fold\",\n",
    "            1: \"Check/Call\",\n",
    "            2: \"Bet/Raise\"\n",
    "        }\n",
    "        \n",
    "        while True:\n",
    "            print(\"\\nAvailable actions:\")\n",
    "            for action in legal_actions:\n",
    "                print(f\"{action}: {action_map[action]}\")\n",
    "            \n",
    "            try:\n",
    "                action = int(input(\"Enter your action (number): \"))\n",
    "                if action in legal_actions:\n",
    "                    return action\n",
    "                print(\"Invalid action! Please choose from available actions.\")\n",
    "            except ValueError:\n",
    "                print(\"Please enter a valid number!\")\n",
    "    \n",
    "    def play_game(self, human_position: int = 0):\n",
    "        \"\"\"Play a full game against the agent\"\"\"\n",
    "        obs = self.env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        print(\"\\nNew game started!\")\n",
    "        print(\"You are in position\", \"SB\" if human_position == 0 else \"BB\")\n",
    "        \n",
    "        while not done:\n",
    "            self.display_game_state(obs)\n",
    "            current_player = self.env.current_player()\n",
    "            \n",
    "            if current_player == human_position:\n",
    "                # Human turn\n",
    "                action = self.get_human_action()\n",
    "            else:\n",
    "                # Agent turn\n",
    "                print(\"\\nAgent is thinking...\")\n",
    "                action = self.agent.act(obs, current_player)\n",
    "                action_map = {0: \"folds\", 1: \"checks/calls\", 2: \"bets/raises\"}\n",
    "                print(f\"Agent {action_map[action]}\")\n",
    "            \n",
    "            obs, reward, done, _ = self.env.step(action)\n",
    "            if done:\n",
    "                total_reward = reward if human_position == 0 else -reward\n",
    "        \n",
    "        # Game over - display results\n",
    "        self.display_game_state(obs)\n",
    "        if total_reward > 0:\n",
    "            print(f\"\\nYou won ${abs(total_reward)}!\")\n",
    "        elif total_reward < 0:\n",
    "            print(f\"\\nYou lost ${abs(total_reward)}\")\n",
    "        else:\n",
    "            print(\"\\nIt's a draw!\")\n",
    "        \n",
    "        return total_reward\n",
    "\n",
    "def play_against_agent(agent: DeepMCCFR, env: Poker, num_games: int = 5):\n",
    "    \"\"\"Play multiple games against the agent\"\"\"\n",
    "    game = PokerGame(agent, env)\n",
    "    total_profit = 0\n",
    "    \n",
    "    for game_num in range(num_games):\n",
    "        print(f\"\\nGame {game_num + 1}/{num_games}\")\n",
    "        # Alternate positions\n",
    "        position = game_num % 2\n",
    "        profit = game.play_game(human_position=position)\n",
    "        total_profit += profit\n",
    "        \n",
    "        print(f\"\\nCurrent profit/loss: ${total_profit}\")\n",
    "        \n",
    "        if game_num < num_games - 1:\n",
    "            input(\"\\nPress Enter to start next game...\")\n",
    "    \n",
    "    print(f\"\\nFinal profit/loss over {num_games} games: ${total_profit}\")\n",
    "    return total_profit\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # First train the agent (assuming you have the training code from before)\n",
    "    env = Poker()\n",
    "    pop_manager = train_population(\n",
    "        num_agents=10,\n",
    "        env=env,\n",
    "        num_iterations=100000\n",
    "    )\n",
    "    \n",
    "    # Get the best performing agent\n",
    "    best_agent_idx = max(\n",
    "        pop_manager.performance_history.keys(),\n",
    "        key=lambda x: np.mean(pop_manager.performance_history[x][-1000:])\n",
    "    )\n",
    "    best_agent = pop_manager.agents[best_agent_idx]\n",
    "    \n",
    "    # Play against the best agent\n",
    "    play_against_agent(best_agent, env, num_games=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
