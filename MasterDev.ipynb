{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poker AI Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pokerenv.obs_indices as indices\n",
    "from pokerenv.table import Table\n",
    "from treys import Deck, Evaluator, Card\n",
    "from pokerenv.common import GameState, PlayerState, PlayerAction, TablePosition, Action, action_list\n",
    "from pokerenv.player import Player\n",
    "from pokerenv.utils import pretty_print_hand, approx_gt, approx_lte\n",
    "import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create enviorment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mod_int_to_str(card_int: int) -> str:\n",
    "        rank_int = Card.get_rank_int(card_int)\n",
    "        suit_int = Card.get_suit_int(card_int)\n",
    "        return Card.STR_RANKS[rank_int] + Card.INT_SUIT_TO_CHAR_SUIT[suit_int]\n",
    "\n",
    "def mod_get_rank_int(card_int: int) -> int:\n",
    "    if card_int is int:\n",
    "        return (card_int >> 8) & 0xF\n",
    "    else:\n",
    "        return (card_int[0] >> 8) & 0xF\n",
    "\n",
    "def mod_get_suit_int(card_int: int) -> int:\n",
    "    if card_int is int:\n",
    "        return (card_int >> 12) & 0xF\n",
    "    else:\n",
    "        return (card_int[0] >> 12) & 0xF\n",
    "\n",
    "def mod_get_bitrank_int(card_int: int) -> int:\n",
    "    if card_int is int:\n",
    "        return (card_int >> 16) & 0x1FFF\n",
    "    else:\n",
    "        return (card_int[0] >> 16) & 0x1FFF\n",
    "\n",
    "def mod_get_prime(card_int: int) -> int:\n",
    "    if card_int is int:\n",
    "        return card_int & 0x3F\n",
    "    else:\n",
    "        return card_int[0] & 0x3F\n",
    "\n",
    "def mod_street_transition(self, transition_to_end=False):\n",
    "        transitioned = False\n",
    "        if self.street == GameState.PREFLOP:\n",
    "            self.cards = self.deck.draw(3)\n",
    "            self._write_event(\"*** FLOP *** [%s %s %s]\" %\n",
    "                              (Card.int_to_str(self.cards[0]), Card.int_to_str(self.cards[1]),\n",
    "                               Card.int_to_str(self.cards[2])))\n",
    "            self.street = GameState.FLOP\n",
    "            transitioned = True\n",
    "        if self.street == GameState.FLOP and (not transitioned or transition_to_end):\n",
    "            new = self.deck.draw(1)[0]\n",
    "            self.cards.append(new)\n",
    "            self._write_event(\"*** TURN *** [%s %s %s] [%s]\" %\n",
    "                              (Card.int_to_str(self.cards[0]), Card.int_to_str(self.cards[1]),\n",
    "                               Card.int_to_str(self.cards[2]), Card.int_to_str(self.cards[3])))\n",
    "            self.street = GameState.TURN\n",
    "            transitioned = True\n",
    "        if self.street == GameState.TURN and (not transitioned or transition_to_end):\n",
    "            new = self.deck.draw(1)[0]\n",
    "            self.cards.append(new)\n",
    "            self._write_event(\"*** RIVER *** [%s %s %s %s] [%s]\" %\n",
    "                              (Card.int_to_str(self.cards[0]), Card.int_to_str(self.cards[1]),\n",
    "                               Card.int_to_str(self.cards[2]), Card.int_to_str(self.cards[3]),\n",
    "                               Card.int_to_str(self.cards[4])))\n",
    "            self.street = GameState.RIVER\n",
    "            transitioned = True\n",
    "        if self.street == GameState.RIVER and (not transitioned or transition_to_end):\n",
    "            if not self.hand_is_over:\n",
    "                if self.hand_history_enabled:\n",
    "                    self._write_show_down()\n",
    "            self.hand_is_over = True\n",
    "        self.street_finished = False\n",
    "        self.last_bet_placed_by = None\n",
    "        self.first_to_act = None\n",
    "        self.bet_to_match = 0\n",
    "        self.minimum_raise = 0\n",
    "        for player in self.players:\n",
    "            player.finish_street()\n",
    "\n",
    "# Create the enviorment:\n",
    "def createEnviorment(active_players, agents, player_names, low_stack_bbs, high_stack_bbs, hand_history_location, invalid_action_penalty, track_single_player=False):\n",
    "    table = Table(active_players, \n",
    "                player_names=player_names,\n",
    "                track_single_player=track_single_player,\n",
    "                stack_low=low_stack_bbs,\n",
    "                stack_high=high_stack_bbs,\n",
    "                hand_history_location=hand_history_location,\n",
    "                invalid_action_penalty=invalid_action_penalty\n",
    "    )\n",
    "    table.seed(1)\n",
    "    \n",
    "\n",
    "    table.int_to_str = mod_int_to_str\n",
    "    table.get_rank_int = mod_get_rank_int\n",
    "    table.get_suit_int = mod_get_suit_int\n",
    "    table.get_bitrank_int = mod_get_bitrank_int\n",
    "    table.get_prime = mod_get_prime\n",
    "    table._street_transition = types.MethodType(mod_street_transition, table)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env Modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learningLoop(table, agents, active_players, n_iterations):\n",
    "    iteration = 1\n",
    "    cumulative_rewards = np.zeros(active_players)\n",
    "    while True:\n",
    "        if iteration % 50 == 0:\n",
    "            table.hand_history_enabled = True\n",
    "        table.n_players = active_players\n",
    "        obs = table.reset()\n",
    "        for agent in agents:\n",
    "            agent.reset()\n",
    "        acting_player = int(obs[indices.ACTING_PLAYER])\n",
    "        while True:\n",
    "            action = agents[acting_player].get_action(obs)\n",
    "            obs, reward, done, _ = table.step(action)\n",
    "            if  done:\n",
    "                # Distribute final rewards\n",
    "                for i in range(active_players):\n",
    "                    agents[i].rewards.append(reward[i])\n",
    "                break\n",
    "            else:\n",
    "                # This step can be skipped unless invalid action penalty is enabled, \n",
    "                # since we only get a reward when the pot is distributed, and the done flag is set\n",
    "                agents[acting_player].rewards.append(reward[acting_player])\n",
    "                acting_player = int(obs[indices.ACTING_PLAYER])\n",
    "        iteration += 1\n",
    "        table.hand_history_enabled = False\n",
    "        \n",
    "        if iteration >= n_iterations:\n",
    "            break\n",
    "    \n",
    "    return iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent that makes random actions\n",
    "class RandomAgent:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def get_action(self, observation):\n",
    "        self.observations.append(observation)\n",
    "        valid_actions = np.argwhere(observation[indices.VALID_ACTIONS] == 1).flatten()\n",
    "        valid_bet_low = observation[indices.VALID_BET_LOW]\n",
    "        valid_bet_high = observation[indices.VALID_BET_HIGH]\n",
    "        chosen_action = PlayerAction(np.random.choice(valid_actions))\n",
    "        bet_size = 0\n",
    "        if chosen_action is PlayerAction.BET:\n",
    "            bet_size = np.random.uniform(valid_bet_low, valid_bet_high)\n",
    "        table_action = Action(chosen_action, bet_size)\n",
    "        self.actions.append(table_action)\n",
    "        return table_action\n",
    "\n",
    "    def reset(self):\n",
    "        self.actions = []\n",
    "        self.observations = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fold Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent that always folds\n",
    "class FoldAgent:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def get_action(self, observation):\n",
    "        self.observations.append(observation)\n",
    "        valid_actions = np.argwhere(observation[indices.VALID_ACTIONS] == 1).flatten()\n",
    "        chosen_action = PlayerAction.FOLD\n",
    "        table_action = Action(chosen_action, 0)\n",
    "        self.actions.append(table_action)\n",
    "        return table_action\n",
    "\n",
    "    def reset(self):\n",
    "        self.actions = []\n",
    "        self.observations = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determined Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent that never folds\n",
    "class DeterminedAgent:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def get_action(self, observation):\n",
    "        self.observations.append(observation)\n",
    "        valid_actions = np.argwhere(observation[indices.VALID_ACTIONS] == 1).flatten()\n",
    "        chosen_action = PlayerAction.CALL\n",
    "        valid_bet_low = observation[indices.VALID_BET_LOW]\n",
    "        valid_bet_high = observation[indices.VALID_BET_HIGH]\n",
    "        table_action = Action(chosen_action, 0)\n",
    "        self.actions.append(table_action)\n",
    "        return table_action\n",
    "\n",
    "    def reset(self):\n",
    "        self.actions = []\n",
    "        self.observations = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggressive agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent that always raises\n",
    "class AggressiveAgent:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def get_action(self, observation):\n",
    "        self.observations.append(observation)\n",
    "        valid_actions = np.argwhere(observation[indices.VALID_ACTIONS] == 1).flatten()\n",
    "        chosen_action = PlayerAction.BET\n",
    "        valid_bet_low = observation[indices.VALID_BET_LOW]\n",
    "        valid_bet_high = observation[indices.VALID_BET_HIGH]\n",
    "        table_action = Action(chosen_action, valid_bet_high)\n",
    "        self.actions.append(table_action)\n",
    "        return table_action\n",
    "\n",
    "    def reset(self):\n",
    "        self.actions = []\n",
    "        self.observations = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chance agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent that uses encoded hand values to make decisions\n",
    "class ValueAgent:\n",
    "    def __init__(self, value_model):\n",
    "        self.actions = []\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "        self.value_model = value_model\n",
    "\n",
    "    def get_action(self, observation):\n",
    "        self.observations.append(observation)\n",
    "        valid_actions = np.argwhere(observation[indices.VALID_ACTIONS] == 1).flatten()\n",
    "        valid_bet_low = observation[indices.VALID_BET_LOW]\n",
    "        valid_bet_high = observation[indices.VALID_BET_HIGH]\n",
    "        hand_value = observation[indices.ACTING_PLAYER_STACK_SIZE]\n",
    "        pot_size = observation[indices.POT_SIZE]\n",
    "        bet_value = self.value_model.predict(np.array([[hand_value, pot_size]]))[0][0]\n",
    "        \n",
    "        if bet_value > 0.5:\n",
    "            chosen_action = PlayerAction.BET\n",
    "            bet_size = np.random.uniform(valid_bet_low, valid_bet_high)\n",
    "        elif valid_bet_high == 0.0:\n",
    "            chosen_action = PlayerAction.CALL\n",
    "        else:\n",
    "            chosen_action = PlayerAction.FOLD\n",
    "        table_action = Action(chosen_action, bet_size)\n",
    "        self.actions.append(table_action)\n",
    "        return table_action\n",
    "\n",
    "    def reset(self):\n",
    "        self.actions = []\n",
    "        self.observations = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Throughs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Run Through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_players = 6\n",
    "agents = [RandomAgent() for _ in range(6)]\n",
    "player_names = {0: 'TrackedAgent1', 1: 'Agent2'} # Rest are defaulted to player3, player4...\n",
    "# Should we only log the 0th players (here TrackedAgent1) private cards to hand history files\n",
    "track_single_player = True \n",
    "# Bounds for randomizing player stack sizes in reset()\n",
    "low_stack_bbs = 50\n",
    "high_stack_bbs = 200\n",
    "hand_history_location = 'hands/'\n",
    "invalid_action_penalty = 0\n",
    "\n",
    "table = createEnviorment(active_players, agents, player_names, low_stack_bbs, high_stack_bbs, hand_history_location, invalid_action_penalty, track_single_player)\n",
    "table.seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_iterations = 1000\n",
    "learningLoop(table, agents, active_players, n_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent rewards:  [[0, 0, 0, 0, 0], [0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0], [0, 0], [0, 0]]\n"
     ]
    }
   ],
   "source": [
    "agent_rewards = [agent.rewards for agent in agents]\n",
    "print(\"Agent rewards: \", agent_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
